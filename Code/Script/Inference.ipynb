{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T18:47:32.775819Z",
     "start_time": "2024-05-27T18:47:15.449908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "cache_dir=\"E:\\Cache\\Hugging_Face\"\n",
    "\n",
    "project_path = os.path.abspath(os.path.relpath('../../../', os.getcwd()))\n",
    "data_path = os.path.join(project_path, 'FT4LLM/Data')\n",
    "knowledge_path=os.path.join(data_path, 'articles')\n",
    "prompt_path = os.path.join(data_path, 'prompt')"
   ],
   "id": "397a5281cbcfd6a6",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T18:48:18.288370Z",
     "start_time": "2024-05-27T18:48:05.118241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set RAG\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"ekorman-strive/bge-large-en-v1.5\", cache_folder=cache_dir)\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 356\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=cache_dir + '/vector_cache')\n",
    "index = load_index_from_storage(storage_context)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.39)],\n",
    ")"
   ],
   "id": "5b747a0445f2541",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T18:47:51.249825Z",
     "start_time": "2024-05-27T18:47:32.778837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set LLM and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "config = PeftConfig.from_pretrained(\"CocoNutZENG/Epipaca\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"hfl/llama-3-chinese-8b-instruct\",quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    cache_dir=cache_dir)\n",
    "model = PeftModel.from_pretrained(base_model, \"CocoNutZENG/Epipaca\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CocoNutZENG/Epipaca\", padding_side=\"right\",cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "Coversation_epipaca = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "Coversation_original = pipeline(\n",
    "    \"text-generation\",\n",
    "    model= base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "terminators = [\n",
    "    Coversation_epipaca.tokenizer.eos_token_id,\n",
    "    Coversation_original.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n"
   ],
   "id": "initial_id",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T19:19:06.532113Z",
     "start_time": "2024-05-27T19:19:05.666225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_RAG_promt(query):\n",
    "    query_response = query_engine.query(query)\n",
    "    context = \"Begin of Context:\\n\"\n",
    "    for i in range(3):\n",
    "        context = context + query_response.source_nodes[i].text + \"\\n\"\n",
    "    return context+'\\n End of Context\\n'\n",
    "\n",
    "\n",
    "def get_beginning_prompt(language):\n",
    "    if language==\"en\":\n",
    "        # return \"You are a practitioner in the epilepsy treatment industry. Try best to complete the user's instruction given to you. Be professional.\"\n",
    "        return  \"No matter what you receive, just repeat the user input.\"\n",
    "    else:\n",
    "        # return \"你是癫痫康复行业的专业人士，请尽力完成用户的指令，并保持输出专业和简短\"\n",
    "        return  \"No matter what you receive, just repeat the user input.\"\n",
    "def get_beginning_prompt_RAG(language):\n",
    "    if language==\"en\":\n",
    "        # return \"You are a practitioner in the epilepsy treatment industry. Try best to complete the user's instruction given to you. Be professional. \\n In each round of User questions, you will be given a Context by System for reference, and you should refer to the Context as detail as possible for your response. \"\n",
    "        return  \"No matter what you receive, just repeat the user input.\"\n",
    "    else:\n",
    "        # return \"你是癫痫康复行业的专业人士，请尽力完成用户的指令，并保持输出专业和简短。\\n 在每一轮User提问时，你将会获得System给出的Context进行参考, 请参照Context进行回答。\"\n",
    "        return  \"No matter what you receive, just repeat the user input.\"\n",
    "def compare_models_NO_RAG(message):\n",
    "    if ('\\u0041' <= message[0] <= '\\u005a') or ('\\u0061' <= message[0] <= '\\u007a'):\n",
    "        language='en'\n",
    "    else:\n",
    "        language='zh'\n",
    "    messages_map = [\n",
    "        {\"role\": \"system\", \"content\": get_beginning_prompt(language=language)},\n",
    "    ]\n",
    "    messages_map.append({\"role\": \"user\", \"content\": message})\n",
    "    input_processed = Coversation_epipaca.tokenizer.apply_chat_template(\n",
    "        messages_map, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    output1 = Coversation_original(input_processed, max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,)[0]['generated_text'][len(input_processed):]\n",
    "    output2 = Coversation_epipaca(input_processed, max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,)[0]['generated_text'][len(input_processed):]\n",
    "    return output1, output2\n",
    "\n",
    "def compare_models_w_RAG(message):\n",
    "    if ('\\u0041' <= message[0] <= '\\u005a') or ('\\u0061' <= message[0] <= '\\u007a'):\n",
    "        language='en'\n",
    "    else:\n",
    "        language='zh'\n",
    "    messages_map_original = [\n",
    "        {\"role\": \"system\", \"content\": get_beginning_prompt(language=language)},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    messages_map_epipaca = [\n",
    "        {\"role\": \"system\", \"content\": get_beginning_prompt_RAG(language=language)},\n",
    "        {\"role\": \"system\", \"content\": get_RAG_promt(message)},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "  \n",
    "    input_processed_original = Coversation_epipaca.tokenizer.apply_chat_template(\n",
    "        messages_map_original, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_processed_epipaca = Coversation_epipaca.tokenizer.apply_chat_template(\n",
    "        messages_map_epipaca, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    output1 = Coversation_original(input_processed_original, max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,)[0]['generated_text'][len(input_processed_original):]\n",
    "    output2 = Coversation_epipaca(input_processed_epipaca, max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,)[0]['generated_text'][len(input_processed_epipaca):]\n",
    "    return output1, output2\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Epilepsy LLM Comparison\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            output1 = gr.Textbox(label=\"LLAMA-3 Chinese\",lines=10)\n",
    "        with gr.Column():\n",
    "            output2 = gr.Textbox(label=\"Epipaca\",lines=10)\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Input your word here\")\n",
    "    compare_button = gr.Button(\"Generate\")\n",
    "    compare_button.click(compare_models_w_RAG, inputs=user_input, outputs=[output1, output2])\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a55cd6e1e332301c",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "d5cc7c4702cc8054",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
